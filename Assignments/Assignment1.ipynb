{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up selenium/ beautifulsoup/ pandas\n",
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create two functions, one for scrapping normal pages and the other for Alaska and American Samoa \n",
    "2. Loop over all pages to scrape required data\n",
    "3. After collecting all data, append to one list, then create dataframe object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The chromedriver version (123.0.6312.58) detected in PATH at c:\\Users\\quent\\repos\\ISOM3400\\Assignments\\chromedriver.exe might not be compatible with the detected chrome version (124.0.6367.63); currently, chromedriver 124.0.6367.91 is recommended for chrome 124.*, so it is advised to delete the driver in PATH and retry\n"
     ]
    }
   ],
   "source": [
    "# Set up selenium and soup \n",
    "service = Service() \n",
    "options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# options = webdriver.ChromeOptions()\n",
    "# options.add_experimental_option(\"detach\", True)\n",
    "# service = Service(executable_path=\"chromedriver.exe\") <-- for users with installed chromedriver in directory \n",
    "# driver = webdriver.Chrome(service=service, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alabama', 'arkansas', 'california', 'colorado', 'iowa', 'maine', 'massachusetts', 'minnesota', 'north carolina', 'oklahoma', 'tennessee', 'texas', 'utah', 'vermont', 'virginia']\n"
     ]
    }
   ],
   "source": [
    "states_democratic = ['Alabama', 'American Samoa', 'Arkansas', 'California', 'Colorado', \n",
    "                     'Iowa', 'Maine', 'Massachusetts', 'Minnesota', 'North Carolina',\n",
    "                     'Oklahoma', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia']\n",
    "states_republican = ['Alabama', 'Alaska', 'Arkansas', 'California', 'Colorado',\n",
    "                     'Maine', 'Massachusetts', 'Minnesota', 'North Carolina', \n",
    "                     'Oklahoma', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia']\n",
    "states_both = [state.lower() for state in states_democratic if state in states_republican]\n",
    "states_both.insert(4, 'iowa')\n",
    "print(states_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for scrapping normal pages \n",
    "def scrape_election_page(state_nm):\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    columns = soup.select('div.tile-h_ymFU.cnn-pcl-13b0kh1[data-testid=\"card\"]')\n",
    "    county_name = ''\n",
    "\n",
    "# Add in delegate numbers from table above\n",
    "    delegate_rows = soup.select('tr.cnn-pcl-1me6450.isWinner-3g_AYM')\n",
    "    candidate_delegates = {} \n",
    "    for row in delegate_rows:\n",
    "        name_element = row.select_one('span[data-testid=\"candidate-name\"]')\n",
    "        candidate = name_element.get_text(strip=True) if name_element else None\n",
    "\n",
    "        delegate_element = row.select_one('td[data-testid=\"delegates\"]')\n",
    "        delegates = int(delegate_element.get_text(strip=True)) if delegate_element else 0\n",
    "\n",
    "        # Initialize the candidate in the dictionary if it doesn't exist\n",
    "        if candidate not in candidate_delegates:\n",
    "            candidate_delegates[candidate] = 0\n",
    "            \n",
    "        # Store the candidate-delegate pair in the dictionary\n",
    "        candidate_delegates[candidate] += delegates\n",
    "\n",
    "\n",
    "    for column in columns: \n",
    "        county_name = soup.select_one('article.core-result div.header-container-1LzJY9 h2').text\n",
    "        table = column.select_one('table.cnn-pcl-1me6450')\n",
    "        rows = table.select('tr.cnn-pcl-1me6450')\n",
    "\n",
    "        for row in rows:\n",
    "            if state_nm not in states_both:\n",
    "                state = 'Iowa'\n",
    "            else: \n",
    "                state = soup.select_one('h2.header-2-AOgLYo.cnn-pcl-xk8c6r').get_text().split()[-1]\n",
    "            \n",
    "            county = county_name\n",
    "            name_element = row.select_one('span[data-testid=\"candidate-name\"]')\n",
    "            candidate = name_element.get_text(strip=True) if name_element else None\n",
    "\n",
    "            party_element = row.select_one('span[data-testid=\"party-label\"]')\n",
    "            party = party_element.get_text(strip=True).split(',')[0] if party_element else None\n",
    "\n",
    "            incumbent = 'Incumbent' in party_element.get_text(strip=True) if party_element else False\n",
    "            incumbent_status = 'Yes' if incumbent else 'No'\n",
    "\n",
    "            vote_percent_element = row.select_one('td[data-testid=\"votepercent\"]')\n",
    "            percentage = vote_percent_element.get_text(strip=True) if vote_percent_element else None\n",
    "            \n",
    "            vote_count_element = row.select_one('span[data-testid=\"votes\"]')\n",
    "            votes = 0\n",
    "            if vote_count_element:\n",
    "                try:\n",
    "                    votes = int(vote_count_element.get_text(strip=True).replace(',', ''))\n",
    "                except ValueError:\n",
    "                    # Handle the case where the text cannot be converted to an integer\n",
    "                    votes = 0\n",
    "            \n",
    "\n",
    "            winner = 'Yes' if incumbent_status == 'Yes' else 'No'\n",
    "\n",
    "            delegates = candidate_delegates.get(candidate, 0)\n",
    "\n",
    "            if candidate and votes:\n",
    "                yield {\n",
    "                    'State': state,\n",
    "                    'County': county_name,\n",
    "                    'Candidate': candidate,\n",
    "                    'Party': party,\n",
    "                    'Incumbent': incumbent_status,\n",
    "                    'Votes': votes,\n",
    "                    'Percentage': percentage,\n",
    "                    'Winner': winner,\n",
    "                    'Delegates': delegates\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for scrapping the two special pages \n",
    "def scrape_election_page_special():\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    columns = soup.select('article.core-result')\n",
    "    \n",
    "    for column in columns:\n",
    "        # print(column.prettify())\n",
    "        # print('-' * 80)\n",
    "        county_name = None\n",
    "        table = column.select_one('table.cnn-pcl-1me6450')\n",
    "        rows = table.select('tr.cnn-pcl-1me6450')\n",
    "\n",
    "        for row in rows:\n",
    "            candidate_delegates = {} \n",
    "            name_element = row.select_one('span[data-testid=\"candidate-name\"]')\n",
    "            candidate = name_element.get_text(strip=True) if name_element else None\n",
    "\n",
    "            delegate_element = row.select_one('td[data-testid=\"delegates\"]')\n",
    "            try:\n",
    "                delegates = int(delegate_element.get_text(strip=True)) if delegate_element else 0\n",
    "            except ValueError:\n",
    "            # Handle the case where the text cannot be converted to an integer\n",
    "                    delegates = 0\n",
    "\n",
    "            # Initialize the candidate in the dictionary if it doesn't exist\n",
    "            if candidate not in candidate_delegates:\n",
    "                candidate_delegates[candidate] = 0\n",
    "                \n",
    "            # Store the candidate-delegate pair in the dictionary\n",
    "            candidate_delegates[candidate] += delegates\n",
    "\n",
    "            state = soup.select_one('h2.header-2-AOgLYo.cnn-pcl-xk8c6r').get_text().split()[-1]\n",
    "            party_element = row.select_one('span[data-testid=\"party-label\"]')\n",
    "            party = party_element.get_text(strip=True).split(',')[0] if party_element else None\n",
    "\n",
    "            incumbent = 'Incumbent' in party_element.get_text(strip=True) if party_element else False\n",
    "            incumbent_status = 'Yes' if incumbent else 'No'\n",
    "\n",
    "            vote_percent_element = row.select_one('td[data-testid=\"votepercent\"]')\n",
    "            percentage = vote_percent_element.get_text(strip=True) if vote_percent_element else None\n",
    "            \n",
    "            vote_count_element = row.select_one('span[data-testid=\"votes\"]')\n",
    "            votes = 0\n",
    "            if vote_count_element:\n",
    "                try:\n",
    "                    votes = int(vote_count_element.get_text(strip=True).replace(',', ''))\n",
    "                except ValueError:\n",
    "                    # Handle the case where the text cannot be converted to an integer\n",
    "                    votes = 0\n",
    "        \n",
    "            winner = 'Yes' if incumbent_status == 'Yes' else 'No'\n",
    "\n",
    "            delegates = candidate_delegates.get(candidate, 0)\n",
    "\n",
    "            if party:\n",
    "                yield {\n",
    "                    'State': state,\n",
    "                    'County': county_name,\n",
    "                    'Candidate': candidate,\n",
    "                    'Party': party,\n",
    "                    'Incumbent': incumbent_status,\n",
    "                    'Votes': votes,\n",
    "                    'Percentage': percentage,\n",
    "                    'Winner': winner,\n",
    "                    'Delegates': delegates\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_dem = \"https://edition.cnn.com/election/2024/primaries-and-caucuses/results/\"\n",
    "suffix_dem = \"/democratic-presidential-primary\"\n",
    "prefix_rep = \"https://edition.cnn.com/election/2024/primaries-and-caucuses/results/\"\n",
    "suffix_rep = \"/republican-presidential-primary\"\n",
    "\n",
    "combined_data = [] \n",
    "columns = ['State', 'County', 'Candidate', 'Party', 'Incumbent', 'Votes', 'Percentage', 'Winner', 'Delegates']\n",
    "combined_df = pd.DataFrame(columns=columns)\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# Append data for Democratic Party \n",
    "for i in states_both:\n",
    "    # modify the state names\n",
    "    state_name = i.replace(\" \", \"-\")\n",
    "    link = f'{prefix_dem}{state_name}{suffix_dem}'\n",
    "    driver.get(link)\n",
    "\n",
    "    wait = WebDriverWait(driver, 1)  \n",
    "    button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.rightButton.cnn-pcl-13b0kh1\")))\n",
    "\n",
    "    # Right-click button element \n",
    "    have_button = True\n",
    "    button = driver.find_element(By.CSS_SELECTOR, \"button.rightButton.cnn-pcl-13b0kh1\")\n",
    "\n",
    "    # Display all pages \n",
    "    while have_button: \n",
    "        try:\n",
    "            button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.rightButton.cnn-pcl-13b0kh1\")))\n",
    "            # Scrape the election page and add data to combined list\n",
    "            for election_data in scrape_election_page(state_name):\n",
    "                    if election_data is not None:\n",
    "                        combined_data.append(election_data) \n",
    "            button.click()\n",
    "        except TimeoutException:\n",
    "            have_button = False\n",
    "\n",
    "# # Scrapping Iowa because for some reason it doesn't work TT\n",
    "# link = 'https://edition.cnn.com/election/2024/primaries-and-caucuses/results/iowa/democratic-presidential-primary'\n",
    "# driver.get(link)\n",
    "# wait = WebDriverWait(driver, 1)  \n",
    "# button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.rightButton.cnn-pcl-13b0kh1\")))\n",
    "\n",
    "# # Right-click button element \n",
    "# have_button = True\n",
    "# button = driver.find_element(By.CSS_SELECTOR, \"button.rightButton.cnn-pcl-13b0kh1\")\n",
    "\n",
    "# # Display all pages \n",
    "# while have_button: \n",
    "#     try:\n",
    "#         button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.rightButton.cnn-pcl-13b0kh1\")))\n",
    "#         # Scrape the election page and add data to combined list\n",
    "#         for election_data in scrape_election_page():\n",
    "#                 if election_data is not None:\n",
    "#                     combined_data.append(election_data) \n",
    "#         button.click()\n",
    "#     except TimeoutException:\n",
    "#         have_button = False\n",
    "\n",
    "\n",
    "\n",
    "# Append data for Republican Party \n",
    "for i in states_both:\n",
    "    # modify the state names\n",
    "    state_name = i.replace(\" \", \"-\")\n",
    "    link = f'{prefix_rep}{state_name}{suffix_rep}'\n",
    "    driver.get(link)\n",
    "\n",
    "    wait = WebDriverWait(driver, 1)  \n",
    "    button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.rightButton.cnn-pcl-13b0kh1\")))\n",
    "\n",
    "    # Right-click button element \n",
    "    have_button = True\n",
    "    button = driver.find_element(By.CSS_SELECTOR, \"button.rightButton.cnn-pcl-13b0kh1\")\n",
    "\n",
    "    # Display all pages \n",
    "    while have_button: \n",
    "        try:\n",
    "            button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.rightButton.cnn-pcl-13b0kh1\")))\n",
    "            # Scrape the election page and add data to combined list\n",
    "            for election_data in scrape_election_page(state_name):\n",
    "                    if election_data is not None:\n",
    "                        combined_data.append(election_data)\n",
    "            button.click()\n",
    "        except TimeoutException:\n",
    "            have_button = False\n",
    "\n",
    "    \n",
    "# Append data for the two special webpages (Alaska and American Samoa)\n",
    "states_special = ['Alaska', 'American Samoa'] \n",
    "for i in states_special: \n",
    "        state_name = i.lower().replace(\" \", \"-\")\n",
    "        prefix = \"https://edition.cnn.com/election/2024/primaries-and-caucuses/results/\"\n",
    "        link = f'{prefix}{state_name}'\n",
    "        driver.get(link)\n",
    "        for election_data in scrape_election_page_special():\n",
    "                        if election_data is not None:\n",
    "                            combined_data.append(election_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10964, 9)\n"
     ]
    }
   ],
   "source": [
    "combined_df = pd.DataFrame(combined_data)     \n",
    "print(combined_df.shape)  \n",
    "combined_df.reset_index(inplace=True, drop=True) # <-- Reset the index of the combined dataFrame\n",
    "combined_df.index = combined_df.index + 1 \n",
    "combined_df = combined_df.fillna(value=pd.NA)\n",
    "combined_df['Votes'] = combined_df['Votes'].fillna(0).astype('int')\n",
    "\n",
    "# combined_df.to_excel('data.xlsx', sheet_name='Assignment_1_Sheet', index=False)  <-- For self-reference\n",
    "combined_df.to_csv('data2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
