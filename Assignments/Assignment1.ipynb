{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up selenium/ beautifulsoup/ pandas\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create two functions, one for scrapping normal pages and the other for Alaska and American Samoa \n",
    "2. Loop over all pages to scrape required data\n",
    "3. After collecting all data, append to one list, then create dataframe object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The chromedriver version (123.0.6312.58) detected in PATH at c:\\Users\\quent\\repos\\ISOM3400\\Assignments\\chromedriver.exe might not be compatible with the detected chrome version (124.0.6367.60); currently, chromedriver 124.0.6367.60 is recommended for chrome 124.*, so it is advised to delete the driver in PATH and retry\n"
     ]
    }
   ],
   "source": [
    "# Set up selenium and soup \n",
    "service = Service() \n",
    "options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# options = webdriver.ChromeOptions()\n",
    "# options.add_experimental_option(\"detach\", True)\n",
    "# service = Service(executable_path=\"chromedriver.exe\")\n",
    "# driver = webdriver.Chrome(service=service, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for scrapping normal pages \n",
    "def scrape_election_page():\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    WebDriverWait(driver, 5).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, 'div.tile-h_ymFU.cnn-pcl-13b0kh1'))\n",
    "        )\n",
    "    columns = soup.select('div.tile-h_ymFU.cnn-pcl-13b0kh1[data-testid=\"card\"]')\n",
    "    county_name = ''\n",
    "\n",
    "# Add in delegate numbers from table above\n",
    "    delegate_rows = soup.select('tr.cnn-pcl-1me6450.isWinner-3g_AYM')\n",
    "    candidate_delegates = {} \n",
    "    for row in delegate_rows:\n",
    "        name_element = row.select_one('span[data-testid=\"candidate-name\"]')\n",
    "        candidate = name_element.get_text(strip=True) if name_element else None\n",
    "\n",
    "        delegate_element = row.select_one('td[data-testid=\"delegates\"]')\n",
    "        delegates = int(delegate_element.get_text(strip=True)) if delegate_element else 0\n",
    "\n",
    "        # Initialize the candidate in the dictionary if it doesn't exist\n",
    "        if candidate not in candidate_delegates:\n",
    "            candidate_delegates[candidate] = 0\n",
    "            \n",
    "        # Store the candidate-delegate pair in the dictionary\n",
    "        candidate_delegates[candidate] += delegates\n",
    "\n",
    "\n",
    "    for column in columns: \n",
    "        county_name = soup.select_one('article.core-result div.header-container-1LzJY9 h2').text\n",
    "        table = column.select_one('table.cnn-pcl-1me6450')\n",
    "        rows = table.select('tr.cnn-pcl-1me6450')\n",
    "\n",
    "        for row in rows:\n",
    "            state = soup.select_one('h2.header-2-AOgLYo.cnn-pcl-xk8c6r').get_text().split()[-1]\n",
    "            county = county_name\n",
    "            name_element = row.select_one('span[data-testid=\"candidate-name\"]')\n",
    "            candidate = name_element.get_text(strip=True) if name_element else None\n",
    "\n",
    "            party_element = row.select_one('span[data-testid=\"party-label\"]')\n",
    "            party = party_element.get_text(strip=True).split(',')[0] if party_element else None\n",
    "\n",
    "            incumbent = 'Incumbent' in party_element.get_text(strip=True) if party_element else False\n",
    "            incumbent_status = 'Yes' if incumbent else 'No'\n",
    "\n",
    "            vote_percent_element = row.select_one('td[data-testid=\"votepercent\"]')\n",
    "            percentage = vote_percent_element.get_text(strip=True) if vote_percent_element else None\n",
    "            \n",
    "            vote_count_element = row.select_one('span[data-testid=\"votes\"]')\n",
    "            votes = vote_count_element.get_text(strip=True) if vote_count_element else None\n",
    "\n",
    "            winner = 'Yes' if incumbent_status == 'Yes' else 'No'\n",
    "\n",
    "            delegates = candidate_delegates.get(candidate, 0)\n",
    "\n",
    "            if candidate is not None and votes is not None:\n",
    "                yield {\n",
    "                    'State': state,\n",
    "                    'County': county_name,\n",
    "                    'Candidate': candidate,\n",
    "                    'Party': party,\n",
    "                    'Incumbent': incumbent_status,\n",
    "                    'Votes': votes,\n",
    "                    'Percentage': percentage,\n",
    "                    'Winner': winner,\n",
    "                    'Delegates': delegates\n",
    "                }\n",
    "            # print(data)\n",
    "            \n",
    "    # return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for scrapping the two special pages \n",
    "def scrape_election_page_special():\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    columns = soup.select('article.core-result')\n",
    "    \n",
    "    for column in columns:\n",
    "        print(column.prettify())\n",
    "        print('-' * 80)\n",
    "        county_name = None\n",
    "        table = column.select_one('table.cnn-pcl-1me6450')\n",
    "        rows = table.select('tr.cnn-pcl-1me6450')\n",
    "\n",
    "        for row in rows:\n",
    "            candidate_delegates = {} \n",
    "            name_element = row.select_one('span[data-testid=\"candidate-name\"]')\n",
    "            candidate = name_element.get_text(strip=True) if name_element else None\n",
    "\n",
    "            delegate_element = row.select_one('td[data-testid=\"delegates\"]')\n",
    "            try:\n",
    "                delegates = int(delegate_element.get_text(strip=True)) if delegate_element else 0\n",
    "            except ValueError:\n",
    "            # Handle the case where the text cannot be converted to an integer\n",
    "                    delegates = 0\n",
    "\n",
    "            # Initialize the candidate in the dictionary if it doesn't exist\n",
    "            if candidate not in candidate_delegates:\n",
    "                candidate_delegates[candidate] = 0\n",
    "                \n",
    "            # Store the candidate-delegate pair in the dictionary\n",
    "            candidate_delegates[candidate] += delegates\n",
    "\n",
    "\n",
    "            state = soup.select_one('h2.header-2-AOgLYo.cnn-pcl-xk8c6r').get_text().split()[-1]\n",
    "            party_element = row.select_one('span[data-testid=\"party-label\"]')\n",
    "            party = party_element.get_text(strip=True).split(',')[0] if party_element else None\n",
    "\n",
    "            incumbent = 'Incumbent' in party_element.get_text(strip=True) if party_element else False\n",
    "            incumbent_status = 'Yes' if incumbent else 'No'\n",
    "\n",
    "            vote_percent_element = row.select_one('td[data-testid=\"votepercent\"]')\n",
    "            percentage = vote_percent_element.get_text(strip=True) if vote_percent_element else None\n",
    "            \n",
    "            vote_count_element = row.select_one('span[data-testid=\"votes\"]')\n",
    "            votes = vote_count_element.get_text(strip=True) if vote_count_element else None\n",
    "\n",
    "            winner = 'Yes' if incumbent_status == 'Yes' else 'No'\n",
    "\n",
    "            delegates = candidate_delegates.get(candidate, 0)\n",
    "\n",
    "            if party is not None:\n",
    "                yield {\n",
    "                    'State': state,\n",
    "                    'County': county_name,\n",
    "                    'Candidate': candidate,\n",
    "                    'Party': party,\n",
    "                    'Incumbent': incumbent_status,\n",
    "                    'Votes': votes,\n",
    "                    'Percentage': percentage,\n",
    "                    'Winner': winner,\n",
    "                    'Delegates': delegates\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alabama', 'Arkansas', 'California', 'Colorado', 'Iowa', 'Maine', 'Massachusetts', 'Minnesota', 'North Carolina', 'Oklahoma', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia']\n"
     ]
    }
   ],
   "source": [
    "states_democratic = ['Alabama', 'American Samoa', 'Arkansas', 'California', 'Colorado', \n",
    "                     'Iowa', 'Maine', 'Massachusetts', 'Minnesota', 'North Carolina',\n",
    "                     'Oklahoma', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia']\n",
    "states_republican = ['Alabama', 'Alaska', 'Arkansas', 'California', 'Colorado',\n",
    "                     'Maine', 'Massachusetts', 'Minnesota', 'North Carolina', \n",
    "                     'Oklahoma', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia']\n",
    "states_both = [state for state in states_democratic if state in states_republican]\n",
    "states_both.insert(4, 'Iowa')\n",
    "print(states_both)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_dem = \"https://edition.cnn.com/election/2024/primaries-and-caucuses/results/\"\n",
    "suffix_dem = \"/democratic-presidential-primary\"\n",
    "prefix_rep = \"https://edition.cnn.com/election/2024/primaries-and-caucuses/results/\"\n",
    "suffix_rep = \"/republican-presidential-primary\"\n",
    "\n",
    "combined_data = [] \n",
    "columns = ['State', 'County', 'Candidate', 'Party', 'Incumbent', 'Votes', 'Percentage', 'Winner', 'Delegates']\n",
    "combined_df = pd.DataFrame(columns=columns)\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# Append data for Democratic Party \n",
    "for i in states_both:\n",
    "    # modify the state names\n",
    "    state_name = i.lower().replace(\" \", \"-\")\n",
    "    link = f'{prefix_dem}{state_name}{suffix_dem}'\n",
    "    driver.get(link)\n",
    "\n",
    "    wait = WebDriverWait(driver, 1)  # Adjust the timeout value as needed\n",
    "    button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.rightButton.cnn-pcl-13b0kh1\")))\n",
    "\n",
    "    # Right-click button element \n",
    "    have_button = True\n",
    "    button = driver.find_element(By.CSS_SELECTOR, \"button.rightButton.cnn-pcl-13b0kh1\")\n",
    "\n",
    "    # Display all pages \n",
    "    while have_button: \n",
    "        try:\n",
    "            button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.rightButton.cnn-pcl-13b0kh1\")))\n",
    "            # Scrape the election page and add data to combined list\n",
    "            for election_data in scrape_election_page():\n",
    "                    if election_data is not None:\n",
    "                        combined_data.append(election_data) \n",
    "            button.click()\n",
    "        except TimeoutException:\n",
    "            have_button = False\n",
    "\n",
    "\n",
    "# Append data for Republican Party \n",
    "for i in states_both:\n",
    "    # modify the state names\n",
    "    state_name = i.lower().replace(\" \", \"-\")\n",
    "    link = f'{prefix_rep}{state_name}{suffix_rep}'\n",
    "    driver.get(link)\n",
    "\n",
    "    wait = WebDriverWait(driver, 1)  # Adjust the timeout value as needed\n",
    "    button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.rightButton.cnn-pcl-13b0kh1\")))\n",
    "\n",
    "    # Right-click button element \n",
    "    have_button = True\n",
    "    button = driver.find_element(By.CSS_SELECTOR, \"button.rightButton.cnn-pcl-13b0kh1\")\n",
    "\n",
    "    # Display all pages \n",
    "    while have_button: \n",
    "        try:\n",
    "            button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.rightButton.cnn-pcl-13b0kh1\")))\n",
    "            # Scrape the election page and add data to combined list\n",
    "            for election_data in scrape_election_page():\n",
    "                    if election_data is not None:\n",
    "                        combined_data.append(election_data)\n",
    "            button.click()\n",
    "        except TimeoutException:\n",
    "            have_button = False\n",
    "\n",
    "    \n",
    "# Append data for the two special webpages (Alaska and American Samoa)\n",
    "states_special = ['Alaska', 'American Samoa'] \n",
    "for i in states_special: \n",
    "        state_name = i.lower().replace(\" \", \"-\")\n",
    "        prefix = \"https://edition.cnn.com/election/2024/primaries-and-caucuses/results/\"\n",
    "        link = f'{prefix}{state_name}'\n",
    "        driver.get(link)\n",
    "        for election_data in scrape_election_page_special():\n",
    "                        if election_data is not None:\n",
    "                            combined_data.append(election_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.DataFrame(combined_data)     \n",
    "print(combined_df.shape)  \n",
    "combined_df.reset_index(inplace=True, drop=True) # <-- Reset the index of the combined dataFrame\n",
    "combined_df.index = combined_df.index + 1 \n",
    "combined_df = combined_df.fillna(value=np.nan)\n",
    "print(combined_df)\n",
    "\n",
    "#combined_df.to_excel('data.xlsx', sheet_name='Assignment_1_Sheet', index=False) # <-- For self-reference\n",
    "combined_df.to_csv('data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
